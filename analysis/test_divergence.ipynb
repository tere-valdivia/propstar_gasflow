{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e5a1c3-b80e-468a-917c-f3f748dbaa4a",
   "metadata": {},
   "source": [
    "# Test: divergence in HC3N structure\n",
    "\n",
    "We analyse if the divergence of the velocity gradients $\\nabla \\cdot \\nabla$v can give us a clue if infall is hiding within all the outflow activity in HC3N. The velocity gradients will tell us how the mass accelerates, the divergence will indicate any changes in this behavior\n",
    "\n",
    "For this, first we need the velocity gradient of HC3N. As there are several components, but most of the cube has only one Gaussian, we will take the pixels that have 1G fits and test them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0cae42-f115-4feb-9707-154e7c0683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.linalg # for initial guesses\n",
    "from scipy.optimize import curve_fit # to obtain the fit with errors\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.wcs import WCS\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d5e019-ed13-4700-9e4b-e7d7338f517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HC3N1gfile = '../bayes_frame/nested-sampling/HC3N/NGC1333-SE-mle-x1_filtered_QA.fits'\n",
    "\n",
    "# to save\n",
    "velgradbasefile = 'vel_grad_{}_HC3N.fits'\n",
    "velgradxfile = velgradbasefile.format('x')\n",
    "e_velgradxfile = velgradbasefile.format('x_unc')\n",
    "velgradyfile = velgradbasefile.format('y')\n",
    "e_velgradyfile = velgradbasefile.format('y_unc')\n",
    "velgradamplfile = velgradbasefile.format('magnitude')\n",
    "e_velgradamplfile = velgradbasefile.format('magnitude_unc')\n",
    "velgradvclfile = velgradbasefile.format('vc')\n",
    "e_velgradvclfile = velgradbasefile.format('vc_unc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29353fe0-a22f-4a70-85f6-8c47cc2b7cb4",
   "metadata": {},
   "source": [
    "## Calculating velocity gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90ccf1-ede2-4345-8f41-a8770424f177",
   "metadata": {},
   "source": [
    "In principle, we need to sample at least 2 beams, so that the values are uncorrelated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe4d01e-3561-4af3-81b6-d24c096a35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running program for a gradient with a width of 2 beams\n",
      "Beam size: 4.9 arcsec x 4.7 arcsec\n",
      "The equivalent radius is 3.7 pixels, the sample radius is 7, needs a minimum of 51.3 pixels\n"
     ]
    }
   ],
   "source": [
    "# load the velocity and its associated uncertainty\n",
    "params, parheader = fits.getdata(HC3N1gfile, header=True)\n",
    "velfield = params[1] # km/s\n",
    "velerror = params[4]\n",
    "# load the sky WCS\n",
    "wcssky = WCS(parheader).celestial\n",
    "# load the beam size\n",
    "beammaj, beammin, bpa = (parheader['BMAJ'], parheader['BMIN'], parheader['BPA']) * u.deg\n",
    "pixsize = np.abs(parheader['CDELT2']) * u.deg\n",
    "nbeams = 2\n",
    "equivradius = np.sqrt(beammaj * beammin / (4 * np.log(2)))  #equivalence with solid angle of beam in radius form\n",
    "equivradiuspix = (equivradius / pixsize).value # radius in pixels\n",
    "sampleradiuspix = int(np.round(nbeams* equivradiuspix, 0))\n",
    "minarea_calc = (np.pi * sampleradiuspix ** 2) / 3 #at least a third of pixels in the area must be available\n",
    "print(\"Running program for a gradient with a width of {} beams\".format(nbeams))\n",
    "pixsize_pc = (((pixsize.to(u.arcsec) * 302).value) * u.au).to(u.pc).value # parsec per pixel\n",
    "pixsize_au = (((pixsize.to(u.arcsec) * 302).value) * u.au).value # au per pixel\n",
    "print(\"Beam size: {0} x {1}\".format(np.round(beammaj.to(u.arcsec), 1), np.round(beammin.to(u.arcsec), 1)))\n",
    "print(\"The equivalent radius is {0} pixels, the sample radius is {1}, needs a minimum of {2} pixels\".format(np.round(equivradiuspix, 1), np.round(sampleradiuspix, 1), np.round(minarea_calc, 1)))\n",
    "\n",
    "# tolerance parameter for the linear algebra\n",
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07246810-0811-40ed-b6d4-72a667051ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane(X, A, B, vc):\n",
    "    xpos, ypos = X\n",
    "    return A*xpos + B*ypos + vc\n",
    "\n",
    "def gradient(scalarfield, e_scalarfield, radius, minarea, pixunit, eps=1e-3):\n",
    "    filled_indices = np.where(~np.isnan(scalarfield))\n",
    "    gx = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    e_gx = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    gy = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    e_gy = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    constant = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    e_constant = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    magnitude = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    e_magnitude = np.zeros(np.shape(scalarfield)) * np.nan\n",
    "    jumped = 0\n",
    "    for y, x in zip(filled_indices[0], filled_indices[1]):\n",
    "        sampleregion = scalarfield[y-radius:y+radius+1, x-radius:x+radius+1]\n",
    "        sampleerror = e_scalarfield[y-radius:y+radius+1, x-radius:x+radius+1]\n",
    "        X,Y = np.meshgrid(np.arange(-radius, radius+1, 1), np.arange(-radius, radius+1, 1))\n",
    "        if (np.shape(sampleregion)[0] != np.shape(sampleregion)[1]): continue\n",
    "        index_sampleregion_filter = np.where(~np.isnan(sampleregion))\n",
    "        if len(index_sampleregion_filter[0]) < minarea:\n",
    "            jumped+=1\n",
    "            # print('jumped {} {}'.format(x, y), index_sampleregion_filter[0])\n",
    "            continue\n",
    "        else:\n",
    "            # we define the X and Y positions\n",
    "            # we need to get the gradient in km/s/pc\n",
    "            X_pc = X[index_sampleregion_filter] * pixunit\n",
    "            Y_pc = Y[index_sampleregion_filter] * pixunit\n",
    "            sampleregion_filtered = sampleregion[index_sampleregion_filter]\n",
    "            sampleerror_filtered = sampleerror[index_sampleregion_filter]\n",
    "            data_sample = np.transpose([X_pc, Y_pc, sampleregion_filtered])\n",
    "\n",
    "            # we obtain the initial guesses\n",
    "            A = np.c_[data_sample[:,0],data_sample[:,1], np.ones(data_sample.shape[0])] # design matrix X, Y, constants\n",
    "            C,_,_,_ = scipy.linalg.lstsq(A, data_sample[:,2])\n",
    "            if np.abs(C[0])< eps or np.abs(C[1])< eps: continue\n",
    "            A0 = C[0]\n",
    "            B0 = C[1]\n",
    "            vc0 = C[2]\n",
    "            # now we fit with the error map as well\n",
    "            popt, pcov = curve_fit(plane, (X_pc, Y_pc), sampleregion_filtered, p0=[A0, B0, vc0], sigma=sampleerror_filtered, absolute_sigma=True) \n",
    "            #values\n",
    "            gx[y, x] = popt[0]\n",
    "            gy[y, x] = popt[1]\n",
    "            constant[y, x] = popt[2]\n",
    "            magnitude[y, x] = np.sqrt(popt[1]**2 + popt[0]**2)\n",
    "            #uncertainties\n",
    "            perr = np.sqrt(np.diag(pcov))\n",
    "            e_gx[y, x] = perr[0]\n",
    "            e_gy[y, x] = perr[1]\n",
    "            e_constant[y, x] = perr[2]\n",
    "            e_magnitude[y, x] = np.sqrt(perr[0]**2 + perr[1]**2)\n",
    "            \n",
    "    values = [gx, gy, constant, magnitude]\n",
    "    errors = [e_gx, e_gy, e_constant, e_magnitude]\n",
    "    return values, errors, jumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f369af42-b264-47b4-bcf6-2eb802654afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(velgradxfile) or not os.path.exists(velgradxfile):\n",
    "    values_grad, errors_grad, jumped_grad = gradient(velfield, velerror, sampleradiuspix, minarea_calc, pixsize_pc, eps=1e-3)\n",
    "    gradheader = wcssky.to_header()\n",
    "    gradheader['BUNIT'] = 'km s-1 pc-1'\n",
    "    velheader = wcssky.to_header()\n",
    "    velheader['BUNIT'] = 'km s-1'\n",
    "    nablax_map = values_grad[0]\n",
    "    nablay_map = values_grad[1]\n",
    "    vc_map = values_grad[2]\n",
    "    absnabla_map = values_grad[3]\n",
    "    e_nablax_map = errors_grad[0]\n",
    "    e_nablay_map = errors_grad[1]\n",
    "    e_vc_map = errors_grad[2]\n",
    "    e_absnabla_map = errors_grad[3]\n",
    "    fits.writeto(velgradxfile, nablax_map, gradheader, overwrite=True)\n",
    "    fits.writeto(e_velgradxfile, e_nablax_map, gradheader, overwrite=True)\n",
    "    fits.writeto(velgradyfile, nablay_map, gradheader, overwrite=True)\n",
    "    fits.writeto(e_velgradyfile, e_nablay_map, gradheader, overwrite=True)\n",
    "    fits.writeto(velgradvclfile, vc_map, velheader, overwrite=True)\n",
    "    fits.writeto(e_velgradvclfile, e_vc_map, velheader, overwrite=True)\n",
    "    fits.writeto(velgradamplfile, absnabla_map, gradheader, overwrite=True)\n",
    "    fits.writeto(e_velgradamplfile, e_absnabla_map, gradheader, overwrite=True)\n",
    "    print('The program jumped '+str(jumped_grad)+ ' pixels that had less than 1/3 available neighbors')\n",
    "    \n",
    "else:\n",
    "    nablax_map, gradheader = fits.getdata(velgradxfile, header=True)\n",
    "    e_nablax_map, gradheader = fits.getdata(e_velgradxfile, header=True)\n",
    "    nablay_map = fits.getdata(velgradyfile)\n",
    "    e_nablay_map = fits.getdata(e_velgradyfile)\n",
    "    vc_map, velheader = fits.getdata(velgradvclfile, header=True)\n",
    "    e_vc_map = fits.getdata(e_velgradvclfile)\n",
    "    absnabla_map = fits.getdata(velgradamplfile)\n",
    "    e_absnabla_map = fits.getdata(e_velgradamplfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93499e-00dc-44b2-8d77-83cff13d9ab6",
   "metadata": {},
   "source": [
    "Now that we calculated the gradients, lets plot them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fb5fad-e4e8-48a3-83af-325fdf5421ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, subplot_kw={'projection': wcssky}, figsize=(10,7))\n",
    "\n",
    "# im = ax[0].imshow(nablax_map, vmin=-50, vmax=50)\n",
    "# fig.colorbar(im, ax=ax[0], location='top')\n",
    "# im2 = ax[1].imshow(nablay_map, vmin=-50, vmax=50)\n",
    "# fig.colorbar(im2, ax=ax[1], location='top')\n",
    "# im3 = ax[2].imshow(absnabla_map, vmin=-50, vmax=50)\n",
    "# fig.colorbar(im3, ax=ax[2], location='top')\n",
    "# im4 = ax[3].imshow(vc_map)\n",
    "# fig.colorbar(im4, ax=ax[3], location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc003e5b-9e7f-4601-a68b-08397a46aba2",
   "metadata": {},
   "source": [
    "The values look right, normally between -10 and 10 km/s/pc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e48f8-4e1d-45bc-97c5-2ce0d22b2fc1",
   "metadata": {},
   "source": [
    "## Divergence of velocity gradient\n",
    "\n",
    "Now that we have the components of the velocity gradients, we can calculate the divergence.\n",
    "\n",
    "Actually, there is a more straightforward way... the divergence of a gradient is called a Laplacian (of a scalar field). So, if we find an operator that calculates the sum of the second derivatives of our field directly, we do not need to calculate the divergence.\n",
    "\n",
    "We test the Laplacian filter with numpy convolution. We would like to compare this to a brute force approach.\n",
    "\n",
    "The brute force approach is to calculate the divergence of the gradient, by using the same function as what we used to calculate the gradients previously.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla \\cdot \\vec{\\nabla v} = \\frac{\\partial^2v}{\\partial x^2} + \\frac{\\partial^2v}{\\partial y^2}\n",
    "\\end{equation}\n",
    "\n",
    "So, we take the maps $\\vec{\\nabla v}_x$ and $\\vec{\\nabla v}_y$, we do the gradients on those, and just take the map for the x direction for $\\nabla(\\vec{\\nabla v}_x)$ and the y direction for $\\nabla(\\vec{\\nabla v}_y)$ and add them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b5a0d5-096d-4d0e-a490-b0e163cfb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "divfile_brute = 'laplacian_velfield_diag_brute_pc.fits'\n",
    "e_divfile_brute = 'laplacian_velfield_diag_brute_pc_unc.fits'\n",
    "divfile_brute_magnitude = 'laplacian_velfield_diag_brute_pc_abs.fits'\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40f3f0e-08e2-4f6b-810d-84932f014998",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(divfile_brute) or overwrite:\n",
    "    values_divx, errors_divx, jumped_divx = gradient(nablax_map, e_nablax_map, sampleradiuspix, minarea_calc, pixsize_pc, eps=1e-3)\n",
    "    values_divy, errors_divy, jumped_divy = gradient(nablay_map, e_nablay_map, sampleradiuspix, minarea_calc, pixsize_pc, eps=1e-3)\n",
    "    vxx = values_divx[0]\n",
    "    vyy = values_divy[1]\n",
    "    e_vxx = errors_divx[0]\n",
    "    e_vyy = errors_divy[1]\n",
    "    laplacian_map = vxx + vyy\n",
    "    e_laplacian_map = e_vxx + e_vyy\n",
    "    divheader = wcssky.to_header()\n",
    "    divheader['BUNIT'] = 'km s-1 pc-2'\n",
    "    fits.writeto(divfile_brute, laplacian_map, divheader, overwrite=True)\n",
    "    fits.writeto(e_divfile_brute, e_laplacian_map, divheader, overwrite=True)\n",
    "else:\n",
    "    laplacian_map, divheader = fits.getdata(divfile_brute, header=True)\n",
    "    e_laplacian_map = fits.getdata(e_divfile_brute)\n",
    "if not os.path.exists(divfile_brute_magnitude) or overwrite:\n",
    "    abs_laplacian_map = np.abs(laplacian_map)\n",
    "    fits.writeto(divfile_brute_magnitude, abs_laplacian_map, divheader, overwrite=True)\n",
    "else:\n",
    "    abs_laplace_map = fits.getdata(divfile_brute_magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b1921-3778-4e69-a23f-52635e7bc9ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d5f73-7fcb-4626-b1cf-f49bb490bdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "754c9f04-8648-414d-bc11-734a43fbf746",
   "metadata": {},
   "source": [
    "Using the laplacian operator did not work when convolved, and has the drawback that we cannot obtain uncertainties from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fef9384-766b-495c-aafb-0d9705def51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from astropy.convolution import convolve, Gaussian2DKernel\n",
    "# from scipy.ndimage import convolve\n",
    "# from scipy.signal import convolve2d\n",
    "# divfile_iso = 'laplacian_velfield_diag_scipy_pc.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4764897-71fa-4939-be78-6de5f44799f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using the default scipy laplacian operator\n",
    "# 0 1 0\n",
    "# 1 -4 1\n",
    "# # 0 1 0\n",
    "\n",
    "# # also, as we need to take independant areas, we need to expand this kernel to the size of two beams (just as we did for the gradient)\n",
    "\n",
    "\n",
    "# if not os.path.exists(divfile_iso) or overwrite:\n",
    "#     kernel_laplace = np.array([[0,1,0], [1,-4,1], [0,1,0]])\n",
    "#     kernel_gaussian = Gaussian2DKernel(x_stddev=sampleradiuspix, y_stddev=sampleradiuspix)\n",
    "#     kernel_combined = convolve2d(kernel_laplace, kernel_gaussian, mode='full', boundary='fill') \n",
    "#     divergence_iso = convolve(velfield, kernel_laplace, mode='constant') # convolution is associative\n",
    "#     divergence_iso_pc = divergence_iso / (pixsize_pc**2) # km/s/au2\n",
    "#     divheader = wcssky.to_header()\n",
    "#     divheader['BUNIT'] = 'km s-1 pc-2'\n",
    "#     fits.writeto(divfile_iso, divergence_iso_pc, divheader, overwrite=True)\n",
    "# else:\n",
    "#     divergence_iso_pc, divheader = fits.getdata(divfile_iso, divheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49806a79-3d6a-442e-91c0-f68a44f95f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
