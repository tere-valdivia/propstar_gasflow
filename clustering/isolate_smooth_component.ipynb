{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2adb9f-73db-44cc-bee2-fa85426496c2",
   "metadata": {},
   "source": [
    "# Grouping of components according to their velocity and intensity\n",
    "\n",
    "This code is taken from Spandan Choudhury's paper for Barnard 5 flow (2023, submitted), with some modifications to optimize it for our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4484c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm.notebook import tqdm # just to see a cute loading bar :) \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca7f8fe-b9bc-48d4-b95f-f16ac5131050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use this code we need to create those arrays\n",
    "HC3N1gfile = '../bayes_frame/nested-sampling/HC3N/NGC1333-SE-mle-x1_filtered_QA.fits'\n",
    "HC3N2gfile = '../bayes_frame/nested-sampling/HC3N/NGC1333-SE-mle-x2_filtered_QA.fits'\n",
    "HC3N3gfile = '../bayes_frame/nested-sampling/HC3N/NGC1333-SE-mle-x3_filtered_QA.fits'\n",
    "npeaksfile = '../bayes_frame/nested-sampling/HC3N/npeaks_cut5_noislands_QA.fits'\n",
    "\n",
    "#master arrays\n",
    "vel_master_file = 'vel_3cmp_master.fits'\n",
    "sig_master_file = 'sig_3cmp_master.fits'\n",
    "tmb_master_file = 'tmb_3cmp_master.fits'\n",
    "radius_sample = 2\n",
    "weight = 0.5\n",
    "\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed900d1e-972a-4429-b418-d62fb1e7245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "headerHC3N = fits.getheader(HC3N1gfile)\n",
    "xsize = headerHC3N['NAXIS1']\n",
    "ysize = headerHC3N['NAXIS2']\n",
    "\n",
    "params1g = fits.getdata(HC3N1gfile)\n",
    "params2g = fits.getdata(HC3N2gfile)\n",
    "params3g = fits.getdata(HC3N3gfile)\n",
    "\n",
    "npeaks_map = fits.getdata(npeaksfile)\n",
    "npeaks_map[np.where(np.isnan(npeaks_map))] = 0\n",
    "mask_npeaksavailable = np.where(npeaks_map!=0, 1, 0)\n",
    "xarray = np.linspace(0, headerHC3N['NAXIS1']-1, headerHC3N['NAXIS1']).astype(int)\n",
    "yarray = np.linspace(0, headerHC3N['NAXIS2']-1, headerHC3N['NAXIS2']).astype(int)\n",
    "XX, YY = np.meshgrid(xarray, yarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766c3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_ar(xc=None, yc=None):\n",
    "    \"\"\"\n",
    "    returns a 2D array with distance to each pixel from a custom centre pixel, (xc, yc)\n",
    "    \"\"\"\n",
    "    d_ar = np.zeros(npeaks_map.shape)\n",
    "    for i in range(d_ar.shape[0]):\n",
    "        for j in range(d_ar.shape[1]):\n",
    "            d_ar[i,j] = np.hypot(xc - j, yc - i)\n",
    "        \n",
    "    return d_ar\n",
    "\n",
    "# def distance_pix(x, y, x0, y0):\n",
    "#     return np.sqrt((x-x0)**2 + (y-y0)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a78fd",
   "metadata": {},
   "source": [
    "### Parameter distance \n",
    "#### $dp = \\sqrt{\\left( w_{\\rm v} \\times \\frac{\\rm \\Delta v_{LSR}}{\\rm v_{norm}} \\right)^2 + \\left( (1 - w_{\\rm v}) \\times \\frac{\\rm \\Delta T_{TM}}{\\rm T_{MB,norm}} \\right)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd40fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_dist(v1=None, v2=None, tmb1=None, tmb2=None, vel_norm=0.3, tmb_norm=3, w_vel=0.5):\n",
    "    \"\"\"\n",
    "    returns normalised combined 'distance' in velocity and T_MB\n",
    "    \n",
    "    v1, v2 : velocities of the two pixels\n",
    "    tmb1, tmb2 : main beam brightness temperatures of the two pixels\n",
    "    \n",
    "    vel_norm : normalisation factor for difference in velocities\n",
    "    tmb_norm : normalisation factor for difference in T_MB\n",
    "    \n",
    "    w_vel : weight for velocity 'distance'. T_MB diff gets weight (1-w)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    vl_dist = np.abs((v1 - v2) / vel_norm)\n",
    "    tmb_dist = np.abs((tmb1 - tmb2) / tmb_norm)\n",
    "    \n",
    "    dist = np.hypot(vl_dist*w_vel, (1-w_vel)*tmb_dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8bf85d",
   "metadata": {},
   "source": [
    "### read in master arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292c0a0",
   "metadata": {},
   "source": [
    "##### * The velocity, velocity dispersion and the $T_{MB}$ for all components are stored in the arrays 'vel_master', 'sig_master', and 'tmb_master' (without any prior sorting)\n",
    "##### * Both arrays have shapes (3, y, x). Each pixels have 3 values, corresponding to the velocity (or $T_{MB}$) of the 3 components in that pixel. The second (and third) values are NaNs where the pixels have only two (or one) components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "headercomp = headerHC3N.copy()\n",
    "headercomp['NAXIS3'] = 6\n",
    "\n",
    "# we change this to add the uncertainties in the master files\n",
    "if os.path.exists(vel_master_file) and os.path.exists(sig_master_file) and os.path.exists(tmb_master_file) and not overwrite:\n",
    "    vel_master = fits.getdata('velmaster_HC3N.fits')\n",
    "    sig_master = fits.getdata('sigmaster_HC3N.fits')\n",
    "    tmb_master = fits.getdata('tmbmaster_HC3N.fits')\n",
    "else:\n",
    "    vel_master = np.array([params3g[1], params3g[4], params3g[7], params3g[10], params3g[13], params3g[16]]) # this gives a shape 6, y, x\n",
    "    sig_master = np.array([params3g[2], params3g[5], params3g[8], params3g[11], params3g[14], params3g[17]])\n",
    "    tmb_master = np.array([params3g[0], params3g[3], params3g[6], params3g[9], params3g[12], params3g[15]])\n",
    "\n",
    "    index_params1g = np.where(npeaks_map == 1)\n",
    "    for y, x in zip(index_params1g[0], index_params1g[1]):\n",
    "        vel_master[0, y, x] = params1g[1, y, x]\n",
    "        vel_master[3, y, x] = params1g[4, y, x]\n",
    "        sig_master[0, y, x] = params1g[2, y, x]\n",
    "        sig_master[3, y, x] = params1g[5, y, x]\n",
    "        tmb_master[0, y, x] = params1g[0, y, x]\n",
    "        tmb_master[3, y, x] = params1g[3, y, x]\n",
    "\n",
    "    index_params2g = np.where(npeaks_map == 2)\n",
    "    for y, x in zip(index_params2g[0], index_params2g[1]):\n",
    "        vel_master[0, y, x] = params2g[1, y, x]\n",
    "        vel_master[3, y, x] = params2g[7, y, x] # uncertainty\n",
    "        sig_master[0, y, x] = params2g[2, y, x]\n",
    "        sig_master[3, y, x] = params2g[8, y, x]# uncertainty\n",
    "        tmb_master[0, y, x] = params2g[0, y, x]\n",
    "        tmb_master[3, y, x] = params2g[6, y, x]# uncertainty\n",
    "        vel_master[1, y, x] = params2g[4, y, x]\n",
    "        vel_master[4, y, x] = params2g[10, y, x]# uncertainty\n",
    "        sig_master[1, y, x] = params2g[5, y, x]\n",
    "        sig_master[4, y, x] = params2g[11, y, x]# uncertainty\n",
    "        tmb_master[1, y, x] = params2g[3, y, x]\n",
    "        tmb_master[4, y, x] = params2g[9, y, x] # uncertainty\n",
    "    fits.writeto('velmaster_HC3N.fits', vel_master, headercomp, overwrite=True)\n",
    "    fits.writeto('sigmaster_HC3N.fits', sig_master, headercomp, overwrite=True)\n",
    "    fits.writeto('tmbmaster_HC3N.fits', tmb_master, headercomp, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52f2f3",
   "metadata": {},
   "source": [
    "### sort the different components based on the combined parameter distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c327444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_nearest_neighbour(x_start=None, y_start=None, mask=None, rad_asign=1, **kargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    sorts the parameter maps based on combined parametric distance \n",
    "    \n",
    "    \n",
    "    inputs:\n",
    "    x_start, y_start : co-ordinates of the pixel to start the sorting         ; required\n",
    "    mask             : mask within which to do the sorting                    ; optional\n",
    "    rad_assign.      : radius (in pix) to define the neighbourhood of a pixel ; default is 1\n",
    "    \n",
    "    further inputs can be passed into para_dist\n",
    "    \n",
    "    outputs :\n",
    "    vel_sig_tmb_1, vel_sig_tmb_2, vel_sig_tmb_3 : arrays with shapes (3,:,:), corresponding to the component \n",
    "                                                  which is kinematically coherent in the largest extent, and the \n",
    "                                                  remaining components (which is to be sorted further), respectively\n",
    "                                                  \n",
    "                                                  each pixel in each of these arrays has three values, the velocity, \n",
    "                                                  velocity dispersion and T_MB at that pixel, for the corresponding \n",
    "                                                  copmponent\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # create arrays to store the components after sorting\n",
    "  \n",
    "    vel_sig_tmb_1 = np.ones((6,)+ npeaks_map.shape) * np.nan # was 3\n",
    "    vel_sig_tmb_2 = np.ones((6,)+ npeaks_map.shape) * np.nan\n",
    "    vel_sig_tmb_3 = np.ones((6,)+ npeaks_map.shape) * np.nan\n",
    "    \n",
    "    # assign the velocity, velocity dispersion and T_MB at the starting pixel\n",
    "    \n",
    "    # vel_sig_tmb_1[:,y_start, x_start] = [vel_master[0, y_start, x_start], sig_master[0, y_start, x_start], \n",
    "    #                                    tmb_master[0, y_start, x_start]]\n",
    "    # we already know that the 1st component is all the values where we have only one gaussian fit\n",
    "    # this will avoid striping in the final result\n",
    "    vel_sig_tmb_1 = np.array([params1g[1], params1g[2], params1g[0], params1g[4], params1g[5], params1g[3]]) # this is the change we added in v_3\n",
    "    \n",
    "    # we make a tracker map to see where we evaluated\n",
    "    evaluated = np.zeros(np.shape(npeaks_map))\n",
    "    evaluated[np.where(npeaks_map==1)] = 1\n",
    "    evaluated[np.where(npeaks_map==0)] = 1 # to avoid evaluating where there are no values\n",
    "    # create distance array with the reference as the starting pixel : \n",
    "    # the value at each pixel is the distance to that pixel from the reference\n",
    "    \n",
    "    dist_to_pixels = dist_ar(xc=x_start, yc=y_start)\n",
    "    \n",
    "    # apply mask if provided, if not apply a basic mask, to ensure no pixel without a good fit is tried\n",
    "    \n",
    "    try:\n",
    "        y_list, x_list = np.where(mask)\n",
    "    except :\n",
    "        y_list, x_list = np.where(tmb_master[0, :, :] > 0)\n",
    "    \n",
    "    # distance to pixels within the mask\n",
    "    dist_1d_array = dist_to_pixels[y_list, x_list]\n",
    "    print('Loaded arrays and calculated distance')\n",
    "    \n",
    "    # sort the indices of the distance array in increasing order\n",
    "    sorted_index = np.argsort(dist_1d_array)\n",
    "    print('Determined sorted indexes, starting calculation...')\n",
    "    \"\"\"\n",
    "    The sorting is done in the following steps :\n",
    "     1. Start from the starting pixel, go to the next pixel in order of distance from the starting pixel, \n",
    "        and continue in this order\n",
    "     \n",
    "     2. Calculate the mean velocity and Tmb within the radius provided in input. In the first iteration, \n",
    "        this is just the velocity and Tmb of the starting pixel\n",
    "       \n",
    "     3. Calculate the parameter distances for all three components with respect to the mean velocity and Tmb \n",
    "        calculated above\n",
    "        \n",
    "     4. Sort the component with the minimum parameter distance to the first array, \n",
    "        and the other two components (if present) to the other two arrays. \n",
    "    \"\"\"\n",
    "    for indx in tqdm(sorted_index): # if you do not have tqdm, use for indx in sorted_index\n",
    "        \n",
    "        xi, yi = int(x_list[indx]), int(y_list[indx]) #this is the pixel we will evaluate\n",
    "        if evaluated[yi, xi]: continue #do not evaluate the same pixel twice\n",
    "#         dist_i = dist_ar(xc=xi, yc=yi) #this step evaluates all the map, can take too long\n",
    "        \n",
    "#         gd_idx = np.where(dist_i <= rad_asign)           \n",
    "        \n",
    "#         vel_1_all = vel_sig_tmb_1[0, :, :]\n",
    "#         tmb_1_all = vel_sig_tmb_1[2, :, :]\n",
    "        # calculate mean reference values for assigmnent\n",
    "        # vel_1_mean = np.nanmean(vel_1_all[gd_idx])\n",
    "        # tmb_1_mean = np.nanmean(tmb_1_all[gd_idx])\n",
    "        # we change it to this\n",
    "        vel_1_mean = np.nanmean(vel_sig_tmb_1[0, yi-rad_asign:yi+rad_asign+1, xi-rad_asign:xi+rad_asign+1])\n",
    "        # this could have problems on the edges but we have no edges\n",
    "        tmb_1_mean = np.nanmean(vel_sig_tmb_1[2, yi-rad_asign:yi+rad_asign+1, xi-rad_asign:xi+rad_asign+1])\n",
    "        \n",
    "        para_dist_1 = para_dist(v1=vel_master[0, yi, xi], tmb1=tmb_master[0, yi, xi], \n",
    "                                v2=vel_1_mean, tmb2=tmb_1_mean, **kargs)\n",
    "        para_dist_2 = para_dist(v1=vel_master[1, yi, xi], tmb1=tmb_master[1, yi, xi], \n",
    "                                v2=vel_1_mean, tmb2=tmb_1_mean, **kargs)\n",
    "        para_dist_3 = para_dist(v1=vel_master[2, yi, xi], tmb1=tmb_master[2, yi, xi], \n",
    "                                v2=vel_1_mean, tmb2=tmb_1_mean, **kargs)\n",
    "        \n",
    "        para_dist_min = np.nanmin([para_dist_1, para_dist_2, para_dist_3])\n",
    "        \n",
    "        if para_dist_1 == para_dist_min:\n",
    "            vel_sig_tmb_1[:, yi, xi] = [vel_master[0, yi, xi], sig_master[0, yi, xi], tmb_master[0, yi, xi], vel_master[3, yi, xi], sig_master[3, yi, xi], tmb_master[3, yi, xi]]\n",
    "            vel_sig_tmb_2[:, yi, xi] = [vel_master[1, yi, xi], sig_master[1, yi, xi], tmb_master[1, yi, xi], vel_master[4, yi, xi], sig_master[4, yi, xi], tmb_master[4, yi, xi]]\n",
    "            vel_sig_tmb_3[:, yi, xi] = [vel_master[2, yi, xi], sig_master[2, yi, xi], tmb_master[2, yi, xi], vel_master[5, yi, xi], sig_master[5, yi, xi], tmb_master[5, yi, xi]]\n",
    "            \n",
    "        elif para_dist_2 == para_dist_min:\n",
    "            vel_sig_tmb_1[:, yi, xi] = [vel_master[1, yi, xi], sig_master[1, yi, xi], tmb_master[1, yi, xi], vel_master[4, yi, xi], sig_master[4, yi, xi], tmb_master[4, yi, xi]]\n",
    "            vel_sig_tmb_2[:, yi, xi] = [vel_master[2, yi, xi], sig_master[2, yi, xi], tmb_master[2, yi, xi], vel_master[5, yi, xi], sig_master[5, yi, xi], tmb_master[5, yi, xi]]\n",
    "            vel_sig_tmb_3[:, yi, xi] = [vel_master[0, yi, xi], sig_master[0, yi, xi], tmb_master[0, yi, xi], vel_master[3, yi, xi], sig_master[3, yi, xi], tmb_master[3, yi, xi]]\n",
    "            \n",
    "        elif para_dist_3 == para_dist_min:\n",
    "\n",
    "            vel_sig_tmb_1[:, yi, xi] = [vel_master[2, yi, xi], sig_master[2, yi, xi], tmb_master[2, yi, xi], vel_master[5, yi, xi], sig_master[5, yi, xi], tmb_master[5, yi, xi]]\n",
    "            vel_sig_tmb_2[:, yi, xi] = [vel_master[0, yi, xi], sig_master[0, yi, xi], tmb_master[0, yi, xi], vel_master[3, yi, xi], sig_master[3, yi, xi], tmb_master[3, yi, xi]]\n",
    "            vel_sig_tmb_3[:, yi, xi] = [vel_master[1, yi, xi], sig_master[1, yi, xi], tmb_master[1, yi, xi], vel_master[4, yi, xi], sig_master[4, yi, xi], tmb_master[4, yi, xi]]\n",
    "        \n",
    "    return vel_sig_tmb_1, vel_sig_tmb_2, vel_sig_tmb_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32411deb",
   "metadata": {},
   "source": [
    "## sort the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c12e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0  km/s\n",
      "1.0 K\n"
     ]
    }
   ],
   "source": [
    "averagevel = np.round(np.nanmean(params1g[1]), 0)\n",
    "averagetemp = np.round(np.nanmean(params1g[0]), 0)\n",
    "print(averagevel, ' km/s')\n",
    "print(averagetemp, 'K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93082374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded arrays and calculated distance\n",
      "Determined sorted indexes, starting calculation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76de31d8be7c4d2cb8aceed288b206d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5317/593607375.py:90: RuntimeWarning: Mean of empty slice\n",
      "  vel_1_mean = np.nanmean(vel_sig_tmb_1[0, yi-rad_asign:yi+rad_asign+1, xi-rad_asign:xi+rad_asign+1])\n",
      "/tmp/ipykernel_5317/593607375.py:92: RuntimeWarning: Mean of empty slice\n",
      "  tmb_1_mean = np.nanmean(vel_sig_tmb_1[2, yi-rad_asign:yi+rad_asign+1, xi-rad_asign:xi+rad_asign+1])\n",
      "/tmp/ipykernel_5317/593607375.py:101: RuntimeWarning: All-NaN axis encountered\n",
      "  para_dist_min = np.nanmin([para_dist_1, para_dist_2, para_dist_3])\n"
     ]
    }
   ],
   "source": [
    "# arr1 corresponds to `component 1' in the paper\n",
    "\n",
    "# starting pixel is selected from a region with only one component, for clear component assignment\n",
    "\n",
    "arr1, arr2, arr3 = sort_nearest_neighbour(x_start=226, y_start=215, mask=mask_npeaksavailable, w_vel=weight, rad_asign=radius_sample, \n",
    "                                             vel_norm=averagevel, tmb_norm=averagetemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91a4cfcb-d398-440c-a5e6-98aac1e86438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we change the order of the arrays because we work with T, vel, sigma instead of vel, sigma, T\n",
    "arr1_mod = np.zeros(np.shape(arr1)) * np.nan\n",
    "arr1_mod[0, :, :] = arr1[2, :, :]\n",
    "arr1_mod[1, :, :] = arr1[0, :, :]\n",
    "arr1_mod[2, :, :] = arr1[1, :, :]\n",
    "arr1_mod[3, :, :] = arr1[5, :, :]\n",
    "arr1_mod[4, :, :] = arr1[3, :, :]\n",
    "arr1_mod[5, :, :] = arr1[4, :, :]\n",
    "\n",
    "arr2_mod = np.zeros(np.shape(arr2)) * np.nan\n",
    "arr2_mod[0, :, :] = arr2[2, :, :]\n",
    "arr2_mod[1, :, :] = arr2[0, :, :]\n",
    "arr2_mod[2, :, :] = arr2[1, :, :]\n",
    "arr2_mod[3, :, :] = arr2[5, :, :]\n",
    "arr2_mod[4, :, :] = arr2[3, :, :]\n",
    "arr2_mod[5, :, :] = arr2[4, :, :]\n",
    "\n",
    "arr3_mod = np.zeros(np.shape(arr3)) * np.nan\n",
    "arr3_mod[0, :, :] = arr3[2, :, :]\n",
    "arr3_mod[1, :, :] = arr3[0, :, :]\n",
    "arr3_mod[2, :, :] = arr3[1, :, :]\n",
    "arr3_mod[3, :, :] = arr3[5, :, :]\n",
    "arr3_mod[4, :, :] = arr3[3, :, :]\n",
    "arr3_mod[5, :, :] = arr3[4, :, :]\n",
    "\n",
    "fits.writeto('cluster1_HC3N_w{0}_r{1}.fits'.format(weight, radius_sample), arr1_mod, headercomp, overwrite=overwrite)\n",
    "fits.writeto('cluster2_HC3N_w{0}_r{1}.fits'.format(weight, radius_sample), arr2_mod, headercomp, overwrite=overwrite)\n",
    "fits.writeto('cluster3_HC3N_w{0}_r{1}.fits'.format(weight, radius_sample), arr3_mod, headercomp, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021927e-9141-420f-b125-c6eba31cc418",
   "metadata": {},
   "source": [
    "This code works, but we know that there are two different layers that in HC3N can be separated. So what we will do in the future is to first use a clustering algorithm to separate the two main bodies of emission and then use spandan's algorithm to assign the straglers to the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e4ec-88bb-400b-9fda-44c408202bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
